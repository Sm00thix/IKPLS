# This README describes how to generate benchmark figures

- Benchmarking a single fit will report the time to fit a single PLS model.
- Benchmarking a cross-validation will report the total time taken to:
  - Fit the model on every training partition.
  - Compute the mean squared error (MSE) for every PLS component on every validation partition.

## Original benchmarks

The benchmarks shown in paper.md originate from `timings/timings.png` which was generated
by reading contents from `timings/timings.csv` which in turn was generated by executing `time_pls.py`
on the machine and with the arguments specified in `paper.md`.

## To make your own benchmarks

### one run

Execute `time_pls.py` with arguments specifying:
- The PLS algorithm to use.
- The number of components to use.
- The number of cross-validation splits to use, if any.
- The number of parallel jobs to use.
- The shapes of X and Y.

For example, to benchmark the fast cross-validation algorithm with the NumPy implementation of IKPLS algorithm #2 using leave-one-out cross-validation with
1 million samples, 500 features, 10 targets, 30 PLS components, using all available CPU cores for parallel cross-validation, execute the following command in your terminal:

```bash
python3 time_pls.py -model fastnp2 -n 1000000 -k 500 -m 10 -n_components 30 -n_splits 1000000 -n_jobs -1
```

This will run the experiment and append the result as a line in `timings/user_timings.csv` - you can override this filename with the `--output` option.
This also means that once a benchmark has finished, you can safely run another, the result of which will be appended to `timings/user_timings.csv`

Execute the following command in your terminal to get an overview and description of the different arguments and their meaning:

```bash
python3 time_pls.py -h
```

### multiple runs

you can use the Jupyter notebook `reproducing-results-notebook.py` to run multiple benchmarks in a row; this uses `timings/timings.csv` to find the set of runs that need to take place, and stores the results in `timings/user_timings.csv`. Results are cached so that you can run the notebook multiple times without re-running the same benchmarks.

**NOTES**

- that you may need to `pip install jupytext` in order to open this file as a notebook in Jupyter;
- if that does not work for you, the file can be also used as a regular Python script if need be.

### produce a plot

After executing the desired benchmarks, execute `plot_timings.py` to generate `timings/user_timings.png` with your benchmark results.

### A note on cross-validation splits

A user can execute `time_pls.py` with any number of cross-validation splits, `n_splits`, and a record of the time taken to execute the experiment will be written to `timings/user_timings.csv`.
However, for plot_timings.py to work as intended, `n_splits` should be either 1 or the same value as `n` for leave-one-out cross-validation. If this is violated - e.g. if `n` is 100 and `n_splits` is 10 - plot_timings.py will incorrectly interpret this entry as a leave-one-out cross-validation entry and show the result in the leave-one-out cross-validation part of timings/user_timings.png.
